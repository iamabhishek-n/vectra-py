import asyncio
import os
import random
from typing import List, AsyncGenerator
from vectra import VectraClient, VectraConfig, ProviderType, ChunkingStrategy, RetrievalStrategy

# --- MOCK CLASSES ---

class MockPrismaClient:
    async def execute_raw(self, query: str, *args):
        print(f"[MockDB] Insert: {query[:50].strip()}... Params: {len(args)}")
        return 1

    async def query_raw(self, query: str, *args):
        # Return a fake search result
        return [
            {
                "content": "RAG combines LLMs with external knowledge.",
                "metadata": {"source": "mock", "chunk_index": 0},
                "score": 0.89
            }
        ]

async def run_simulation():
    print("=== Starting Vectra SDK Simulation (Python) ===\n")

    # 1. Configuration
    config = VectraConfig(
        embedding={
            "provider": ProviderType.OPENAI,
            "api_key": "mock-key",
            "model_name": "text-embedding-3-small"
        },
        llm={
            "provider": ProviderType.ANTHROPIC, 
            "api_key": "mock-key",
            "model_name": "claude-3-haiku"
        },
        chunking={
            "strategy": ChunkingStrategy.RECURSIVE
        },
        database={
            "type": "prisma",
            "client_instance": MockPrismaClient(),
            "table_name": "Document"
        },
        retrieval={
            "strategy": RetrievalStrategy.HYBRID # Test Hybrid
        }
    )

    print("Initializing Client...")
    client = VectraClient(config)

    # --- MONKEY PATCHING FOR SIMULATION ---
    
    async def mock_embed_docs(texts: List[str]):
        return [[random.random() for _ in range(10)] for _ in texts]
    
    async def mock_embed_query(text: str):
        return [random.random() for _ in range(10)]

    async def mock_generate(prompt: str, sys: str = ""):
        return "This is a simulated answer generated by the Python RAG SDK."

    async def mock_generate_stream(prompt: str, sys: str = "") -> AsyncGenerator[str, None]:
        words = ["This", " is", " a", " streamed", " response."]
        for w in words:
            await asyncio.sleep(0.1)
            yield w

    # Apply patches
    client.embedder.embed_documents = mock_embed_docs
    client.embedder.embed_query = mock_embed_query
    client.llm.generate = mock_generate
    client.llm.generate_stream = mock_generate_stream

    # 2. Query (Standard)
    print(f"\n--- Step 1: Standard Query (Hybrid) ---")
    try:
        result = await client.query_rag("What is RAG?")
        print(f"Answer: {result['answer']}")
    except Exception as e:
        print(f"Query failed: {e}")

    # 3. Query (Streaming)
    print(f"\n--- Step 2: Streaming Query ---")
    try:
        stream = await client.query_rag("Explain...", stream=True)
        print("Stream Output: ", end="")
        async for chunk in stream:
            print(chunk, end="", flush=True)
        print("\n")
    except Exception as e:
        print(f"Streaming failed: {e}")

if __name__ == "__main__":
    asyncio.run(run_simulation())
